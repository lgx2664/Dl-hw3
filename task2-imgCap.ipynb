{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "For this task, we will use the **Flickr8k** dataset. \n",
    "\n",
    "<span style=\"color:red\">TODO:</span>\n",
    "\n",
    "Download the dataset from [here](https://drive.google.com/file/d/1RPYOmaFutLJxrcXr4cfLVQMJ9tKh_ER6/view?usp=sharing) in zip format into the assignment's root directory (DO NOT extract from zip, as a cell below contains the code to extract data).\n",
    "\n",
    "<span style=\"color:red\">NOTE:</span>\n",
    "\n",
    "It is highly likely that you will use VM instance on GCP through jupyter interface for this task. You may download the zip file to your local machine and upload the same zip file to a VM instance through the jupyter interface.\n",
    "\n",
    "**Flickr8k.token.txt** - the raw captions of the Flickr8k Dataset. The first column is the ID of the caption which contains \"image address # caption number\".\n",
    "\n",
    "**Flickr_8k.trainImages.txt** - The training images used in our experiments.\n",
    "\n",
    "**Flickr_8k.testImages.txt** - The test images used in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install zipfile37\n",
    "! pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.imgCap import load_images_list\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Extract Data\n",
    "if not os.path.exists('./Flickr_Data.zip'):\n",
    "    raise Exception('Dataset not found. Please read instructions above this cell and download dataset.')\n",
    "\n",
    "if not os.path.exists('./Flickr_Data'):\n",
    "    print(\"Extracting data ...\")\n",
    "    ZipFile('./Flickr_Data.zip', 'r').extractall('./')\n",
    "\n",
    "#File Containing captions of each image\n",
    "descriptions_file = './Flickr_Data/Flickr8k_text/Flickr8k.token.txt'\n",
    "\n",
    "#Files with names of corresponding images\n",
    "train_image_list_path = './Flickr_Data/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "test_image_list_path = './Flickr_Data/Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "\n",
    "train_image_list = load_images_list(train_image_list_path)\n",
    "test_image_list = load_images_list(test_image_list_path)\n",
    "\n",
    "print('Total train images:',len(train_image_list))\n",
    "print('Total test images:', len(test_image_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoding\n",
    "Extract features from images using InceptionV3 with imagenet weights and transform features into numpy arrays. **2nd layer from last** has the dimension (2048,) which represents features from an image. If you wish to use any other network to produce image encodings, you may do so (make sure to take care of input and output dimensions if you change the network).\n",
    "\n",
    "<span style=\"color:red\">TODO:</span>\n",
    "\n",
    "1. Complete the function images_preprocess_generator in **./utils/imgCap.py**. \n",
    "2. Then **create a model according to the instructions given in the cell below**.\n",
    "\n",
    "<span style=\"color:red\">NOTE:</span> \n",
    "\n",
    "This process takes time and we thus save and reuse the encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import Model\n",
    "from utils.imgCap import images_preprocess_generator\n",
    "import numpy as np\n",
    "\n",
    "##################################################################################################\n",
    "#TODO: Load InceptionV3 with \"imagenet\" weights. Create a model with input and output as follows.\n",
    "# Input : input of first layer of InceptionV3\n",
    "# Output: output of second layer from the end of InceptionV3 network.\n",
    "# Hint: Take a look at tf.keras.Model and Model.input, Model.layers, Model.layers[i].output\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/applications/InceptionV3\n",
    "###################################################################################################\n",
    "# model = \n",
    "# encoder = \n",
    "###################################################################################################\n",
    "# END TODO\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "images_path = './Flickr_Data/Flickr8k_Dataset'\n",
    "\n",
    "encodings_path = './encoded_images'\n",
    "if not os.path.exists(encodings_path):\n",
    "    os.mkdir(encodings_path)\n",
    "\n",
    "#Encode Train Images\n",
    "try:\n",
    "    train_encodings = np.load(encodings_path+\"/train_encodings.npy\")\n",
    "    print('train images encodings found and loaded')\n",
    "except:\n",
    "    print('train images encodings not found. Initializing encoding process...')\n",
    "    processed_train_images_generator = images_preprocess_generator(train_image_list,images_path)\n",
    "    train_encodings = encoder.predict(processed_train_images_generator)\n",
    "    np.save('./encoded_images/train_encodings.npy',train_encodings)\n",
    "    print('train images encodings saved at '+encodings_path)\n",
    "\n",
    "#Encode Test Images\n",
    "try:\n",
    "    test_encodings = np.load(encodings_path+\"/test_encodings.npy\")\n",
    "    print('Test images encodings found and loaded')\n",
    "except:\n",
    "    print('Test images encodings not found. Initializing encoding process...')\n",
    "    processed_test_images_generator = images_preprocess_generator(test_image_list,images_path)\n",
    "    test_encodings = encoder.predict(processed_test_images_generator)\n",
    "    np.save('./encoded_images/test_encodings.npy',test_encodings)\n",
    "    print('Test images encodings saved at '+encodings_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare text\n",
    "Load Descriptions from Flickr8k.token.txt and create Vocabulary from captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.imgCap import load_descriptions, clean_descriptions, generate_vocabulary, word_indexing\n",
    "descriptions_file = './Flickr_Data/Flickr8k_text/Flickr8k.token.txt'\n",
    "descriptions_dict, MAX_TEXT_LENGTH = clean_descriptions(load_descriptions(descriptions_file))\n",
    "vocab = generate_vocabulary(descriptions_dict)\n",
    "word_to_id, id_to_word = word_indexing(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Data Generator\n",
    "\n",
    "The network that we will design in the next cells will have two inputs (one corresponds to an image and the other corresponds to a part of the caption), and it will have one output.\n",
    "\n",
    "Each (image,caption) is converted into multiple (image_encodings, input_caption, output) pairs. For example, consider image 1.png having a caption 'seqstart this is an example seqend' and its corresponding image_encoding numpy array is represented by enc1.\n",
    "\n",
    "Following shows all the data points that correspond to a single (image,caption)\n",
    "\n",
    "| image_encoding | input                                        |  output  |\n",
    "|----------------|----------------------------------------------|----------|\n",
    "|       enc1     |[['seqstart']]                                | 'this'   |  \n",
    "|       enc1     |[['seqstart', 'this']]                        | 'is'     |\n",
    "|       enc1     |[['seqstart', 'this', 'is']]                  | 'an'     |\n",
    "|       enc1     |[['seqstart', 'this', 'is', 'an']]            | 'example'|\n",
    "|       enc1     |[['seqstart', 'this', 'is', 'an', 'example']] | 'seqend' |\n",
    "\n",
    "Thus each (image,caption) pair corresponds to len(caption)-1 number of training points. In cells below  train_generator is written in this fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(max_text_len,vocab_size,word_to_id,train_encodings,batch_size=128):\n",
    "    input_text_array = []\n",
    "    input_image_array = []\n",
    "    output_array = []\n",
    "    while True:\n",
    "        for (index,image_name) in enumerate(train_image_list):\n",
    "            for caption in descriptions_dict[image_name]:\n",
    "                caption_split = caption.split()\n",
    "                for i in range(len(caption_split)-1):\n",
    "                    input_temp = np.zeros(max_text_len)\n",
    "                    input_temp[:i+1] = [word_to_id[caption_split[k]] for k in range(i+1)]\n",
    "                    input_text_array.append(input_temp)\n",
    "\n",
    "                    input_image_array.append(train_encodings[index])\n",
    "\n",
    "                    output_temp = np.zeros(vocab_size)\n",
    "                    output_temp[word_to_id[caption_split[i+1]]] = 1\n",
    "                    output_array.append(output_temp)\n",
    "\n",
    "                    if len(input_text_array) == batch_size:\n",
    "                        yield ([np.array(input_image_array),np.array(input_text_array)],np.array(output_array))\n",
    "                        input_text_array = []\n",
    "                        input_image_array = []\n",
    "                        output_array = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "Now that we have image encodings and the corresponding text, our model should take two inputs and generate one output (as discussed above).\n",
    "\n",
    "- Input 1 - (batch_size,2048) - corresponds to image encodings\n",
    "- Input 2 - (batch_size,max_caption_length,) - corresponds to an array representing words of caption in the form of indices (word_to_id dictionary)\n",
    "\n",
    "- Output - (batch_size,Vocab_size) - gives probability of each word given the image encodings and the sequence at any given time.\n",
    "\n",
    "As we want our network to remember the context, using the LSTM cell is a good idea. Below is one such implementation of an Image Caption Generator.\n",
    "\n",
    "<span style=\"color:red\">TODO:</span>\n",
    "\n",
    "Modify the network to hit the training accuracy of 35%. You may choose to come up with a completely different architecture, if you wish to.\n",
    "\n",
    "Tips:\n",
    "\n",
    "- Try adding more LSTM layers and dense layers. Looking at the official documentation of imported layers is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition \n",
    "VOCAB_SIZE = len(vocab)+1 # Think why +1 is added. Answer: index 0 correspond to unknown word or word not present case. This has to be accomodated in the model.\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, Embedding, Add, Bidirectional, Concatenate, RepeatVector\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Image Features Path\n",
    "image_input = Input(shape=(2048,))\n",
    "image_features = Dense(EMBEDDING_DIM, activation='relu')(image_input)\n",
    "image_features = RepeatVector(MAX_TEXT_LENGTH)(image_features)\n",
    "\n",
    "# Text Features Path\n",
    "text_input = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "text_features = Embedding(VOCAB_SIZE, EMBEDDING_DIM)(text_input)\n",
    "##################################################################################################\n",
    "#TODO: Improve text_features by passing through LSTM layer(s)\n",
    "###################################################################################################\n",
    "\n",
    "###################################################################################################\n",
    "# END TODO\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Combined Features Path\n",
    "combined_features = Concatenate()([image_features,text_features])\n",
    "##################################################################################################\n",
    "#TODO: Improve combined_features by passing through LSTM layer(s)\n",
    "###################################################################################################\n",
    "\n",
    "###################################################################################################\n",
    "# END TODO\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "outputs = Dense(VOCAB_SIZE, activation='softmax')(combined_features)\n",
    "\n",
    "\n",
    "captionGeneratorNet = Model(inputs=[image_input,text_input],outputs=outputs)\n",
    "captionGeneratorNet.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "captionGeneratorNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "generator = train_generator(MAX_TEXT_LENGTH,VOCAB_SIZE,word_to_id,train_encodings)\n",
    "steps = len(train_image_list) * MAX_TEXT_LENGTH // batch_size\n",
    "captionGeneratorNet.fit(generator,verbose=1,epochs=30,steps_per_epoch=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captionGeneratorNet.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Decoder\n",
    "\n",
    "We have a trained model that is capable of generating captions upon looking at images. This process happens sequentially as follows. For example, say enc_trail is a numpy array corresponding to image named trail.png. Now we feed the model the following input\n",
    "\n",
    "Input 1 - enc_train. Shape - (1,2048)\n",
    "\n",
    "Input 2 - [word_to_id[['seqstart']], 0, 0 ........]. Shape - (1,max_caption_length).\n",
    "\n",
    "This will generate an output array of dimensions (Vocab_size) where each entry represent the probability of occurance corresponding to the words in vocabulary. We take argmax to get the max probable next word id, and then convert it back to the word. Say that the word is 'example'. Now we update Input 2 as [word_to_id[['seqstart']], word_to_id[['example']], 0 ........]. This process continues till we hit the max_caption_length or 'seqend'.\n",
    "\n",
    "The code below is written for you, to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_decoder(enc_image): \n",
    "    id_sentence = [word_to_id['seqstart']]\n",
    "    while True:\n",
    "        temp_input = np.zeros(MAX_TEXT_LENGTH)\n",
    "        temp_input[:len(id_sentence)] = id_sentence\n",
    "        next_word_id = np.argmax(captionGeneratorNet.predict([enc_image.reshape(1,2048),temp_input.reshape(1,MAX_TEXT_LENGTH)]))\n",
    "        id_sentence.append(next_word_id)\n",
    "        if len(id_sentence) == MAX_TEXT_LENGTH or next_word_id == word_to_id['seqend']:\n",
    "            out_seq = [id_to_word[ele] for ele in id_sentence]\n",
    "            return out_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Process of Caption Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_image(images_path,image_name):\n",
    "    image = PIL.Image.open(os.path.join(images_path, image_name))\n",
    "    plt.imshow(np.asarray(image.resize((299,299))) / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 0\n",
    "show_image(images_path,test_image_list[ID])\n",
    "image_decoder(test_encodings[ID])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
